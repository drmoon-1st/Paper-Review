{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c515853a",
   "metadata": {},
   "source": [
    "#  StarGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc6c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3a7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        conv_block = [\n",
    "            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n",
    "        ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "    \n",
    "class GeneratorResnet(nn.Module):\n",
    "    def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n",
    "        super(GeneratorResnet, self).__init__()\n",
    "        channels, img_size, _ = img_shape\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv2d(channels+c_dim, 64, 7, stride=1, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        \n",
    "        curr_dim = 64\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=2, bias=False),\n",
    "                nn.InstanceNorm2d(curr_dim * 2, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            curr_dim *= 2\n",
    "            \n",
    "        for _ in range(res_blocks):\n",
    "            model += [ResidualBlock(curr_dim)]\n",
    "            \n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(curr_dim ,curr_dim // 2, 4, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            curr_dim = curr_dim//2\n",
    "            \n",
    "        model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, c), 1)\n",
    "        return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, img_size, _ = img_shape\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n",
    "            return layers\n",
    "        \n",
    "        layers = discriminator_block(channels, 64)\n",
    "        curr_dim = 64\n",
    "        for _ in range(n_strided-1):\n",
    "            layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n",
    "            curr_dim *= 2\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n",
    "        kernel_size = img_size // 2 ** n_strided\n",
    "        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        feature_repr = self.model(img)\n",
    "        out_adv = self.out1(feature_repr)\n",
    "        out_cls = self.out2(feature_repr)\n",
    "        return out_adv, out_cls.view(out_cls.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dafb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, mode=\"train\", attributes=None):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        \n",
    "        self.selected_attrs = attributes\n",
    "        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n",
    "        self.files = self.files[:-2000] if mode == \"train\" else self.files[-2000:]\n",
    "        self.label_path = glob.glob(\"%s/*.txt\" % root)[0]\n",
    "        self.annotations = self.get_annotations()\n",
    "        \n",
    "    def get_annotations(self):\n",
    "        annotations = {}\n",
    "        lines = [line.rstrip() for line in open(self.label_path, \"r\")]\n",
    "        self.label_names = lines[1].split()\n",
    "        for _, line in enumerate(lines[2:]):\n",
    "            filename, *values = lne.split()\n",
    "            labels = []\n",
    "            for attr in self.selected_attrs:\n",
    "                idx = self.label_names.index(attr)\n",
    "                labels.append(1 * values(values[idx] == \"1\"))\n",
    "            annotations[filename] = labels\n",
    "            return annotations\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.files[index % len(self.files)]\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        img = self.transform(Image.open(filepath))\n",
    "        label = self.annotations[filename]\n",
    "        label = torch.FloatTensor(np.array(label))\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed552966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=0, n_epoch=200, dataset_name='img_align_celeba', batch_size=16, lr=0.0002, b1=0.5, b2=0.999, decay_epoch=100, n_cpu=8, img_height=128, img_width=128, channels=3, sample_interval=400, checkpoint_interval=-1, residual_blocks=6, selected_attrs=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'], n_critic=5)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n",
    "parser.add_argument(\"--n_epoch\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"img_align_celeba\", help=\"name of the dataset\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoc from witch to start lr decay\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads toues during batch generation\")\n",
    "parser.add_argument(\"--img_height\", type=int, default=128, help=\"size of image height\")\n",
    "parser.add_argument(\"--img_width\", type=int, default=128, help=\"size of image width\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between saving generator samples\")\n",
    "parser.add_argument(\"--checkpoint_interval\", type=int, default=-1, help=\"interval between model checkpoints\")\n",
    "parser.add_argument(\"--residual_blocks\", type=int, default=6, help=\"number of residual blocks in generator\")\n",
    "parser.add_argument(\n",
    "    \"--selected_attrs\",\n",
    "    \"--list\",\n",
    "    nargs=\"+\",\n",
    "    help=\"selected attributes for the CelebA dataset\",\n",
    "    default=[\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Male\", \"Young\"],\n",
    ")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training iterations for WGAN discriminator\")\n",
    "opt, _ = parser.parse_known_args()\n",
    "print(opt)\n",
    "\n",
    "c_dim = len(opt.selected_attrs)\n",
    "img_shape = (opt.channels, opt.img_height, opt.img_width)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "\n",
    "def criterion_cls(logit, target):\n",
    "    return F.binary_cross_entrtopy_with_logits(logit, target, size_average=False) / logit.size(0)\n",
    "\n",
    "generator = GeneratorResnet(img_shape=img_shape, res_blocks=opt.residual_blocks, c_dim=c_dim)\n",
    "discriminator = Discriminator(img_shape=img_shape, c_dim=c_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33dc497",
   "metadata": {},
   "source": [
    "Code : https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/stargan/stargan.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
